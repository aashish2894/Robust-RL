{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aashishkumar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/aashishkumar/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "#import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "from scores.score_logger import ScoreLogger\n",
    "\n",
    "EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_ddqn.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_next = self.model.predict(update_target)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_next[i])\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Run: 1, exploration: 0.986090636999001, score: 14\n",
      "Scores: (min: 14, avg: 14, max: 14)\n",
      "\n",
      "Run: 2, exploration: 0.9723747443770956, score: 14\n",
      "Scores: (min: 14, avg: 14, max: 14)\n",
      "\n",
      "Run: 3, exploration: 0.9540649618417361, score: 19"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aashishkumar/Documents/Projects/Robust-RL/scores/score_logger.py:77: RankWarning: Polyfit may be poorly conditioned\n",
      "  z = np.polyfit(np.array(trend_x), np.array(y[1:]), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores: (min: 14, avg: 15.666666666666666, max: 19)\n",
      "\n",
      "Run: 4, exploration: 0.9407945259609451, score: 14\n",
      "Scores: (min: 14, avg: 15.25, max: 19)\n",
      "\n",
      "Run: 5, exploration: 0.9138900318559524, score: 29\n",
      "Scores: (min: 14, avg: 18, max: 29)\n",
      "\n",
      "Run: 6, exploration: 0.8948890480710096, score: 21\n",
      "Scores: (min: 14, avg: 18.5, max: 29)\n",
      "\n",
      "Run: 7, exploration: 0.8754068367770318, score: 22\n",
      "Scores: (min: 14, avg: 19, max: 29)\n",
      "\n",
      "Run: 8, exploration: 0.847823709077432, score: 32\n",
      "Scores: (min: 14, avg: 20.625, max: 32)\n",
      "\n",
      "Run: 9, exploration: 0.8301963316171974, score: 21\n",
      "Scores: (min: 14, avg: 20.666666666666668, max: 32)\n",
      "\n",
      "Run: 10, exploration: 0.8088788946494789, score: 26\n",
      "Scores: (min: 14, avg: 21.2, max: 32)\n",
      "\n",
      "Run: 11, exploration: 0.8000255819490795, score: 11\n",
      "Scores: (min: 11, avg: 20.272727272727273, max: 32)\n",
      "\n",
      "Run: 12, exploration: 0.7920612314455105, score: 10\n",
      "Scores: (min: 10, avg: 19.416666666666668, max: 32)\n",
      "\n",
      "Run: 13, exploration: 0.7771467460721305, score: 19\n",
      "Scores: (min: 10, avg: 19.384615384615383, max: 32)\n",
      "\n",
      "Run: 14, exploration: 0.7534131012276413, score: 31\n",
      "Scores: (min: 10, avg: 20.214285714285715, max: 32)\n",
      "\n",
      "Run: 15, exploration: 0.7399663251239436, score: 18\n",
      "Scores: (min: 10, avg: 20.066666666666666, max: 32)\n",
      "\n",
      "Run: 16, exploration: 0.7202448110511043, score: 27\n",
      "Scores: (min: 10, avg: 20.5, max: 32)\n",
      "\n",
      "Run: 17, exploration: 0.7137884761549381, score: 9\n",
      "Scores: (min: 9, avg: 19.823529411764707, max: 32)\n",
      "\n",
      "Run: 18, exploration: 0.7024531167280339, score: 16\n",
      "Scores: (min: 9, avg: 19.61111111111111, max: 32)\n",
      "\n",
      "Run: 19, exploration: 0.6947646516921667, score: 11\n",
      "Scores: (min: 9, avg: 19.157894736842106, max: 32)\n",
      "\n",
      "Run: 20, exploration: 0.6762478484398523, score: 27\n",
      "Scores: (min: 9, avg: 19.55, max: 32)\n",
      "\n",
      "Run: 21, exploration: 0.6675091808180759, score: 13\n",
      "Scores: (min: 9, avg: 19.238095238095237, max: 32)\n",
      "\n",
      "Run: 22, exploration: 0.6516718490384363, score: 24\n",
      "Scores: (min: 9, avg: 19.454545454545453, max: 32)\n",
      "\n",
      "Run: 23, exploration: 0.6445391933396063, score: 11\n",
      "Scores: (min: 9, avg: 19.08695652173913, max: 32)\n",
      "\n",
      "Run: 24, exploration: 0.6298767245155701, score: 23\n",
      "Scores: (min: 9, avg: 19.25, max: 32)\n",
      "\n",
      "Run: 25, exploration: 0.6229826200436561, score: 11\n",
      "Scores: (min: 9, avg: 18.92, max: 32)\n",
      "\n",
      "Run: 26, exploration: 0.6130893082982078, score: 16\n",
      "Scores: (min: 9, avg: 18.807692307692307, max: 32)\n",
      "\n",
      "Run: 27, exploration: 0.5979446000009478, score: 25\n",
      "Scores: (min: 9, avg: 19.037037037037038, max: 32)\n",
      "\n",
      "Run: 28, exploration: 0.5913999978901242, score: 11\n",
      "Scores: (min: 9, avg: 18.75, max: 32)\n",
      "\n",
      "Run: 29, exploration: 0.5785249048906184, score: 22\n",
      "Scores: (min: 9, avg: 18.862068965517242, max: 32)\n",
      "\n",
      "Run: 30, exploration: 0.5704779919833761, score: 14\n",
      "Scores: (min: 9, avg: 18.7, max: 32)\n",
      "\n",
      "Run: 31, exploration: 0.5653641794148941, score: 9\n",
      "Scores: (min: 9, avg: 18.387096774193548, max: 32)\n",
      "\n",
      "Run: 32, exploration: 0.5552736652928869, score: 18\n",
      "Scores: (min: 9, avg: 18.375, max: 32)\n",
      "\n",
      "Run: 33, exploration: 0.5464556095429605, score: 16\n",
      "Scores: (min: 9, avg: 18.303030303030305, max: 32)\n",
      "\n",
      "Run: 34, exploration: 0.5404745629114154, score: 11\n",
      "Scores: (min: 9, avg: 18.08823529411765, max: 32)\n",
      "\n",
      "Run: 35, exploration: 0.5345589798201541, score: 11\n",
      "Scores: (min: 9, avg: 17.885714285714286, max: 32)\n",
      "\n",
      "Run: 36, exploration: 0.5239687596143511, score: 20\n",
      "Scores: (min: 9, avg: 17.944444444444443, max: 32)\n",
      "\n",
      "Run: 37, exploration: 0.5156478432405085, score: 16\n",
      "Scores: (min: 9, avg: 17.89189189189189, max: 32)\n",
      "\n",
      "Run: 38, exploration: 0.5100039926842729, score: 11\n",
      "Scores: (min: 9, avg: 17.710526315789473, max: 32)\n",
      "\n",
      "Run: 39, exploration: 0.5039174930867489, score: 12\n",
      "Scores: (min: 9, avg: 17.564102564102566, max: 32)\n",
      "\n",
      "Run: 40, exploration: 0.4836647671103911, score: 41\n",
      "Scores: (min: 9, avg: 18.15, max: 41)\n",
      "\n",
      "Run: 41, exploration: 0.46983310087889063, score: 29\n",
      "Scores: (min: 9, avg: 18.414634146341463, max: 41)\n",
      "\n",
      "Run: 42, exploration: 0.4637617835123936, score: 13\n",
      "Scores: (min: 9, avg: 18.285714285714285, max: 41)\n",
      "\n",
      "Run: 43, exploration: 0.4514016473420717, score: 27\n",
      "Scores: (min: 9, avg: 18.488372093023255, max: 41)\n",
      "\n",
      "Run: 44, exploration: 0.44735524511437874, score: 9\n",
      "Scores: (min: 9, avg: 18.272727272727273, max: 41)\n",
      "\n",
      "Run: 45, exploration: 0.4429017700604677, score: 10\n",
      "Scores: (min: 9, avg: 18.08888888888889, max: 41)\n",
      "\n",
      "Run: 46, exploration: 0.4363045472783448, score: 15\n",
      "Scores: (min: 9, avg: 18.02173913043478, max: 41)\n",
      "\n",
      "Run: 47, exploration: 0.43152912216191214, score: 11\n",
      "Scores: (min: 9, avg: 17.872340425531913, max: 41)\n",
      "\n",
      "Run: 48, exploration: 0.427233198057808, score: 10\n",
      "Scores: (min: 9, avg: 17.708333333333332, max: 41)\n",
      "\n",
      "Run: 49, exploration: 0.42213450329202495, score: 12\n",
      "Scores: (min: 9, avg: 17.591836734693878, max: 41)\n",
      "\n",
      "Run: 50, exploration: 0.41751417164041027, score: 11\n",
      "Scores: (min: 9, avg: 17.46, max: 41)\n",
      "\n",
      "Run: 51, exploration: 0.41170681546900234, score: 14\n",
      "Scores: (min: 9, avg: 17.392156862745097, max: 41)\n",
      "\n",
      "Run: 52, exploration: 0.4059802359226587, score: 14\n",
      "Scores: (min: 9, avg: 17.326923076923077, max: 41)\n",
      "\n",
      "Run: 53, exploration: 0.40033330944997925, score: 14\n",
      "Scores: (min: 9, avg: 17.264150943396228, max: 41)\n",
      "\n",
      "Run: 54, exploration: 0.39516008821566384, score: 13\n",
      "Scores: (min: 9, avg: 17.185185185185187, max: 41)\n",
      "\n",
      "Run: 55, exploration: 0.39161784004119166, score: 9\n",
      "Scores: (min: 9, avg: 17.036363636363635, max: 41)\n",
      "\n",
      "Run: 56, exploration: 0.38578451466104513, score: 15\n",
      "Scores: (min: 9, avg: 17, max: 41)\n",
      "\n",
      "Run: 57, exploration: 0.380799297103564, score: 13\n",
      "Scores: (min: 9, avg: 16.92982456140351, max: 41)\n",
      "\n",
      "Run: 58, exploration: 0.37700839448485435, score: 10\n",
      "Scores: (min: 9, avg: 16.810344827586206, max: 41)\n",
      "\n",
      "Run: 59, exploration: 0.37400286247792053, score: 8\n",
      "Scores: (min: 8, avg: 16.661016949152543, max: 41)\n",
      "\n",
      "Run: 60, exploration: 0.3691698907910935, score: 13\n",
      "Scores: (min: 8, avg: 16.6, max: 41)\n",
      "\n",
      "Run: 61, exploration: 0.36586062092624033, score: 9\n",
      "Scores: (min: 8, avg: 16.475409836065573, max: 41)\n",
      "\n",
      "Run: 62, exploration: 0.36185621618376534, score: 11\n",
      "Scores: (min: 8, avg: 16.387096774193548, max: 41)\n",
      "\n",
      "Run: 63, exploration: 0.35789564031060395, score: 11\n",
      "Scores: (min: 8, avg: 16.3015873015873, max: 41)\n",
      "\n",
      "Run: 64, exploration: 0.3424821096413374, score: 44\n",
      "Scores: (min: 8, avg: 16.734375, max: 44)\n",
      "\n",
      "Run: 65, exploration: 0.3380564581151032, score: 13\n",
      "Scores: (min: 8, avg: 16.676923076923078, max: 44)\n",
      "\n",
      "Run: 66, exploration: 0.33469106557869915, score: 10\n",
      "Scores: (min: 8, avg: 16.575757575757574, max: 44)\n",
      "\n",
      "Run: 67, exploration: 0.33135917592813624, score: 10\n",
      "Scores: (min: 8, avg: 16.47761194029851, max: 44)\n",
      "\n",
      "Run: 68, exploration: 0.32707725812456445, score: 13\n",
      "Scores: (min: 8, avg: 16.426470588235293, max: 44)\n",
      "\n",
      "Run: 69, exploration: 0.3222052939901265, score: 15\n",
      "Scores: (min: 8, avg: 16.405797101449274, max: 44)\n",
      "\n",
      "Run: 70, exploration: 0.3189977016914014, score: 10\n",
      "Scores: (min: 8, avg: 16.314285714285713, max: 44)\n",
      "\n",
      "Run: 71, exploration: 0.3155062193168902, score: 11\n",
      "Scores: (min: 8, avg: 16.239436619718308, max: 44)\n",
      "\n",
      "Run: 72, exploration: 0.3120529517919121, score: 11\n",
      "Scores: (min: 8, avg: 16.166666666666668, max: 44)\n",
      "\n",
      "Run: 73, exploration: 0.30925568295888084, score: 9\n",
      "Scores: (min: 8, avg: 16.068493150684933, max: 44)\n",
      "\n",
      "Run: 74, exploration: 0.306177005589209, score: 10\n",
      "Scores: (min: 8, avg: 15.986486486486486, max: 44)\n",
      "\n",
      "Run: 75, exploration: 0.30282584784472627, score: 11\n",
      "Scores: (min: 8, avg: 15.92, max: 44)\n",
      "\n",
      "Run: 76, exploration: 0.2995113690735936, score: 11\n",
      "Scores: (min: 8, avg: 15.855263157894736, max: 44)\n",
      "\n",
      "Run: 77, exploration: 0.2965296975159237, score: 10\n",
      "Scores: (min: 8, avg: 15.779220779220779, max: 44)\n",
      "\n",
      "Run: 78, exploration: 0.2938715804362218, score: 9\n",
      "Scores: (min: 8, avg: 15.692307692307692, max: 44)\n",
      "\n",
      "Run: 79, exploration: 0.2900740880362934, score: 13\n",
      "Scores: (min: 8, avg: 15.658227848101266, max: 44)\n",
      "\n",
      "Run: 80, exploration: 0.2866122801967262, score: 12\n",
      "Scores: (min: 8, avg: 15.6125, max: 44)\n",
      "\n",
      "Run: 81, exploration: 0.2840430636776882, score: 9\n",
      "Scores: (min: 8, avg: 15.530864197530864, max: 44)\n",
      "\n",
      "Run: 82, exploration: 0.2800922055970794, score: 14\n",
      "Scores: (min: 8, avg: 15.512195121951219, max: 44)\n",
      "\n",
      "Run: 83, exploration: 0.2770265502839066, score: 11\n",
      "Scores: (min: 8, avg: 15.457831325301205, max: 44)\n",
      "\n",
      "Run: 84, exploration: 0.2696432642822646, score: 27\n",
      "Scores: (min: 8, avg: 15.595238095238095, max: 44)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 85, exploration: 0.26536117873480936, score: 16\n",
      "Scores: (min: 8, avg: 15.6, max: 44)\n",
      "\n",
      "Run: 86, exploration: 0.262719476412822, score: 10\n",
      "Scores: (min: 8, avg: 15.534883720930232, max: 44)\n",
      "\n",
      "Run: 87, exploration: 0.260364436990891, score: 9\n",
      "Scores: (min: 8, avg: 15.459770114942529, max: 44)\n",
      "\n",
      "Run: 88, exploration: 0.2577724778315252, score: 10\n",
      "Scores: (min: 8, avg: 15.397727272727273, max: 44)\n",
      "\n",
      "Run: 89, exploration: 0.25495111561414624, score: 11\n",
      "Scores: (min: 8, avg: 15.348314606741573, max: 44)\n",
      "\n",
      "Run: 90, exploration: 0.2526657124299791, score: 9\n",
      "Scores: (min: 8, avg: 15.277777777777779, max: 44)\n",
      "\n",
      "Run: 91, exploration: 0.25065144723887983, score: 8\n",
      "Scores: (min: 8, avg: 15.197802197802197, max: 44)\n",
      "\n",
      "Run: 92, exploration: 0.2481561820560168, score: 10\n",
      "Scores: (min: 8, avg: 15.141304347826088, max: 44)\n",
      "\n",
      "Run: 93, exploration: 0.24421532336806764, score: 16\n",
      "Scores: (min: 8, avg: 15.150537634408602, max: 44)\n",
      "\n",
      "Run: 94, exploration: 0.23604755704357036, score: 34\n",
      "Scores: (min: 8, avg: 15.351063829787234, max: 44)\n",
      "\n",
      "Run: 95, exploration: 0.23276428588715223, score: 14\n",
      "Scores: (min: 8, avg: 15.336842105263157, max: 44)\n",
      "\n",
      "Run: 96, exploration: 0.2238568488524224, score: 39\n",
      "Scores: (min: 8, avg: 15.583333333333334, max: 44)\n",
      "\n",
      "Run: 97, exploration: 0.20976214758481407, score: 65\n",
      "Scores: (min: 8, avg: 16.09278350515464, max: 65)\n",
      "\n",
      "Run: 98, exploration: 0.20274666558487714, score: 34\n",
      "Scores: (min: 8, avg: 16.275510204081634, max: 65)\n",
      "\n",
      "Run: 99, exploration: 0.17642356102382997, score: 139\n",
      "Scores: (min: 8, avg: 17.515151515151516, max: 139)\n",
      "\n",
      "Run: 100, exploration: 0.1461734072526738, score: 188\n",
      "Scores: (min: 8, avg: 19.22, max: 188)\n",
      "\n",
      "Run: 101, exploration: 0.12567737250057, score: 151\n",
      "Scores: (min: 8, avg: 20.59, max: 188)\n",
      "\n",
      "Run: 102, exploration: 0.11776434669602452, score: 65\n",
      "Scores: (min: 8, avg: 21.1, max: 188)\n",
      "\n",
      "Run: 103, exploration: 0.11012896007524788, score: 67\n",
      "Scores: (min: 8, avg: 21.58, max: 188)\n",
      "\n",
      "Run: 104, exploration: 0.10496511190976955, score: 48\n",
      "Scores: (min: 8, avg: 21.92, max: 188)\n",
      "\n",
      "Run: 105, exploration: 0.0994446302532099, score: 54\n",
      "Scores: (min: 8, avg: 22.17, max: 188)\n",
      "\n",
      "Run: 106, exploration: 0.09216339036195662, score: 76\n",
      "Scores: (min: 8, avg: 22.72, max: 188)\n",
      "\n",
      "Run: 107, exploration: 0.08431151526911018, score: 89\n",
      "Scores: (min: 8, avg: 23.39, max: 188)\n",
      "\n",
      "Run: 108, exploration: 0.07454902132312784, score: 123\n",
      "Scores: (min: 8, avg: 24.3, max: 188)\n",
      "\n",
      "Run: 109, exploration: 0.06704767127628951, score: 106\n",
      "Scores: (min: 8, avg: 25.15, max: 188)\n",
      "\n",
      "Run: 110, exploration: 0.058284362418015906, score: 140\n",
      "Scores: (min: 8, avg: 26.29, max: 188)\n",
      "\n",
      "Run: 111, exploration: 0.05236719237745592, score: 107\n",
      "Scores: (min: 8, avg: 27.25, max: 188)\n",
      "\n",
      "Run: 112, exploration: 0.0434315759578471, score: 187\n",
      "Scores: (min: 8, avg: 29.02, max: 188)\n",
      "\n",
      "Run: 113, exploration: 0.03925725231456919, score: 101\n",
      "Scores: (min: 8, avg: 29.84, max: 188)\n",
      "\n",
      "Run: 114, exploration: 0.033718955224492995, score: 152\n",
      "Scores: (min: 8, avg: 31.05, max: 188)\n",
      "\n",
      "Run: 115, exploration: 0.02636235555853626, score: 246\n",
      "Scores: (min: 8, avg: 33.33, max: 246)\n",
      "\n",
      "Run: 116, exploration: 0.021495314876978793, score: 204\n",
      "Scores: (min: 8, avg: 35.1, max: 246)\n",
      "\n",
      "Run: 117, exploration: 0.01675522945186236, score: 249\n",
      "Scores: (min: 8, avg: 37.5, max: 249)\n",
      "\n",
      "Run: 118, exploration: 0.012712329172892809, score: 276\n",
      "Scores: (min: 8, avg: 40.1, max: 276)\n",
      "\n",
      "Run: 119, exploration: 0.009998671593271896, score: 280\n",
      "Scores: (min: 8, avg: 42.79, max: 280)\n",
      "\n",
      "Run: 120, exploration: 0.009998671593271896, score: 303\n",
      "Scores: (min: 8, avg: 45.55, max: 303)\n",
      "\n",
      "Run: 121, exploration: 0.009998671593271896, score: 309\n",
      "Scores: (min: 8, avg: 48.51, max: 309)\n",
      "\n",
      "Run: 122, exploration: 0.009998671593271896, score: 269\n",
      "Scores: (min: 8, avg: 50.96, max: 309)\n",
      "\n",
      "Run: 123, exploration: 0.009998671593271896, score: 269\n",
      "Scores: (min: 8, avg: 53.54, max: 309)\n",
      "\n",
      "Run: 124, exploration: 0.009998671593271896, score: 302\n",
      "Scores: (min: 8, avg: 56.33, max: 309)\n",
      "\n",
      "Run: 125, exploration: 0.009998671593271896, score: 324\n",
      "Scores: (min: 8, avg: 59.46, max: 324)\n",
      "\n",
      "Run: 126, exploration: 0.009998671593271896, score: 296\n",
      "Scores: (min: 8, avg: 62.26, max: 324)\n",
      "\n",
      "Run: 127, exploration: 0.009998671593271896, score: 318\n",
      "Scores: (min: 8, avg: 65.19, max: 324)\n",
      "\n",
      "Run: 128, exploration: 0.009998671593271896, score: 290\n",
      "Scores: (min: 8, avg: 67.98, max: 324)\n",
      "\n",
      "Run: 129, exploration: 0.009998671593271896, score: 272\n",
      "Scores: (min: 8, avg: 70.48, max: 324)\n",
      "\n",
      "Run: 130, exploration: 0.009998671593271896, score: 265\n",
      "Scores: (min: 8, avg: 72.99, max: 324)\n",
      "\n",
      "Run: 131, exploration: 0.009998671593271896, score: 255\n",
      "Scores: (min: 8, avg: 75.45, max: 324)\n",
      "\n",
      "Run: 132, exploration: 0.009998671593271896, score: 276\n",
      "Scores: (min: 8, avg: 78.03, max: 324)\n",
      "\n",
      "Run: 133, exploration: 0.009998671593271896, score: 302\n",
      "Scores: (min: 8, avg: 80.89, max: 324)\n",
      "\n",
      "Run: 134, exploration: 0.009998671593271896, score: 325\n",
      "Scores: (min: 8, avg: 84.03, max: 325)\n",
      "\n",
      "Run: 135, exploration: 0.009998671593271896, score: 256\n",
      "Scores: (min: 8, avg: 86.48, max: 325)\n",
      "\n",
      "Run: 136, exploration: 0.009998671593271896, score: 288\n",
      "Scores: (min: 8, avg: 89.16, max: 325)\n",
      "\n",
      "Run: 137, exploration: 0.009998671593271896, score: 354\n",
      "Scores: (min: 8, avg: 92.54, max: 354)\n",
      "\n",
      "Run: 138, exploration: 0.009998671593271896, score: 300\n",
      "Scores: (min: 8, avg: 95.43, max: 354)\n",
      "\n",
      "Run: 139, exploration: 0.009998671593271896, score: 298\n",
      "Scores: (min: 8, avg: 98.29, max: 354)\n",
      "\n",
      "Run: 140, exploration: 0.009998671593271896, score: 306\n",
      "Scores: (min: 8, avg: 100.94, max: 354)\n",
      "\n",
      "Run: 141, exploration: 0.009998671593271896, score: 349\n",
      "Scores: (min: 8, avg: 104.14, max: 354)\n",
      "\n",
      "Run: 142, exploration: 0.009998671593271896, score: 322\n",
      "Scores: (min: 8, avg: 107.23, max: 354)\n",
      "\n",
      "Run: 143, exploration: 0.009998671593271896, score: 343\n",
      "Scores: (min: 8, avg: 110.39, max: 354)\n",
      "\n",
      "Run: 144, exploration: 0.009998671593271896, score: 316\n",
      "Scores: (min: 8, avg: 113.46, max: 354)\n",
      "\n",
      "Run: 145, exploration: 0.009998671593271896, score: 491\n",
      "Scores: (min: 8, avg: 118.27, max: 491)\n",
      "\n",
      "Run: 146, exploration: 0.009998671593271896, score: 400\n",
      "Scores: (min: 8, avg: 122.12, max: 491)\n",
      "\n",
      "Run: 147, exploration: 0.009998671593271896, score: 421\n",
      "Scores: (min: 8, avg: 126.22, max: 491)\n",
      "\n",
      "Run: 148, exploration: 0.009998671593271896, score: 391\n",
      "Scores: (min: 8, avg: 130.03, max: 491)\n",
      "\n",
      "Run: 149, exploration: 0.009998671593271896, score: 395\n",
      "Scores: (min: 8, avg: 133.86, max: 491)\n",
      "\n",
      "Run: 150, exploration: 0.009998671593271896, score: 454\n",
      "Scores: (min: 8, avg: 138.29, max: 491)\n",
      "\n",
      "Run: 151, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 143.15, max: 500)\n",
      "\n",
      "Run: 152, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 148.01, max: 500)\n",
      "\n",
      "Run: 153, exploration: 0.009998671593271896, score: 448\n",
      "Scores: (min: 8, avg: 152.35, max: 500)\n",
      "\n",
      "Run: 154, exploration: 0.009998671593271896, score: 38\n",
      "Scores: (min: 8, avg: 152.6, max: 500)\n",
      "\n",
      "Run: 155, exploration: 0.009998671593271896, score: 353\n",
      "Scores: (min: 8, avg: 156.04, max: 500)\n",
      "\n",
      "Run: 156, exploration: 0.009998671593271896, score: 411\n",
      "Scores: (min: 8, avg: 160, max: 500)\n",
      "\n",
      "Run: 157, exploration: 0.009998671593271896, score: 105\n",
      "Scores: (min: 8, avg: 160.92, max: 500)\n",
      "\n",
      "Run: 158, exploration: 0.009998671593271896, score: 101\n",
      "Scores: (min: 8, avg: 161.83, max: 500)\n",
      "\n",
      "Run: 159, exploration: 0.009998671593271896, score: 58\n",
      "Scores: (min: 8, avg: 162.33, max: 500)\n",
      "\n",
      "Run: 160, exploration: 0.009998671593271896, score: 44\n",
      "Scores: (min: 8, avg: 162.64, max: 500)\n",
      "\n",
      "Run: 161, exploration: 0.009998671593271896, score: 67\n",
      "Scores: (min: 8, avg: 163.22, max: 500)\n",
      "\n",
      "Run: 162, exploration: 0.009998671593271896, score: 64\n",
      "Scores: (min: 8, avg: 163.75, max: 500)\n",
      "\n",
      "Run: 163, exploration: 0.009998671593271896, score: 60\n",
      "Scores: (min: 8, avg: 164.24, max: 500)\n",
      "\n",
      "Run: 164, exploration: 0.009998671593271896, score: 89\n",
      "Scores: (min: 8, avg: 164.69, max: 500)\n",
      "\n",
      "Run: 165, exploration: 0.009998671593271896, score: 291\n",
      "Scores: (min: 8, avg: 167.47, max: 500)\n",
      "\n",
      "Run: 166, exploration: 0.009998671593271896, score: 276\n",
      "Scores: (min: 8, avg: 170.13, max: 500)\n",
      "\n",
      "Run: 167, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 175.03, max: 500)\n",
      "\n",
      "Run: 168, exploration: 0.009998671593271896, score: 246\n",
      "Scores: (min: 8, avg: 177.36, max: 500)\n",
      "\n",
      "Run: 169, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 182.21, max: 500)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 170, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 187.11, max: 500)\n",
      "\n",
      "Run: 171, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 192, max: 500)\n",
      "\n",
      "Run: 172, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 196.89, max: 500)\n",
      "\n",
      "Solved in 72 runs, 172 total runs.\n",
      "Run: 173, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 201.8, max: 500)\n",
      "\n",
      "Solved in 73 runs, 173 total runs.\n",
      "Run: 174, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 206.7, max: 500)\n",
      "\n",
      "Solved in 74 runs, 174 total runs.\n",
      "Run: 175, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 211.59, max: 500)\n",
      "\n",
      "Solved in 75 runs, 175 total runs.\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)\n",
    "# get size of state and action from environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "score_logger = ScoreLogger(ENV_NAME)\n",
    "\n",
    "agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "scores, episodes = [], []\n",
    "run = 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    step = 0\n",
    "    run += 1\n",
    "    while not done:\n",
    "        step += 1\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # get action for the current state and go one step in environment\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "        #reward = reward if not done or score == 499 else -100\n",
    "        reward = reward if not done else -reward\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            #score = score if score == 500 else score + 100\n",
    "            #scores.append(score)\n",
    "            \n",
    "            print(\"Run: \" + str(run) + \", exploration: \" + str(agent.epsilon) + \", score: \" + str(step))\n",
    "            score_logger.add_score(step, run)\n",
    "                \n",
    "#             episodes.append(e)\n",
    "#             pylab.plot(episodes, scores, 'b')\n",
    "#             pylab.savefig(\"./save_graph/cartpole_ddqn.png\")\n",
    "#             print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "#                   len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 490\n",
    "            # stop training\n",
    "#             if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "#                 sys.exit()\n",
    "\n",
    "    # save the model\n",
    "    # if e % 50 == 0:\n",
    "    #     agent.model.save_weights(\"./save_model/cartpole_ddqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save_weights(\"my_model_no_noisy_reward.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
