{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "#import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "from scores.score_logger import ScoreLogger\n",
    "\n",
    "EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_ddqn.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_next = self.model.predict(update_target)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_next[i])\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Run: 1, exploration: 0.986090636999001, score: 14\n",
      "Scores: (min: 14, avg: 14, max: 14)\n",
      "\n",
      "Run: 2, exploration: 0.9723747443770956, score: 14\n",
      "Scores: (min: 14, avg: 14, max: 14)\n",
      "\n",
      "Run: 3, exploration: 0.9531108968798944, score: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aashishkumar/Documents/Projects/Robust-RL/scores/score_logger.py:77: RankWarning: Polyfit may be poorly conditioned\n",
      "  z = np.polyfit(np.array(trend_x), np.array(y[1:]), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: (min: 14, avg: 16, max: 20)\n",
      "\n",
      "Run: 4, exploration: 0.9267809647166116, score: 28\n",
      "Scores: (min: 14, avg: 19, max: 28)\n",
      "\n",
      "Run: 5, exploration: 0.9066044494080763, score: 22\n",
      "Scores: (min: 14, avg: 19.6, max: 28)\n",
      "\n",
      "Run: 6, exploration: 0.8815592697443159, score: 28\n",
      "Scores: (min: 14, avg: 21, max: 28)\n",
      "\n",
      "Run: 7, exploration: 0.8684280445126921, score: 15\n",
      "Scores: (min: 14, avg: 20.142857142857142, max: 28)\n",
      "\n",
      "Run: 8, exploration: 0.8512234991370281, score: 20\n",
      "Scores: (min: 14, avg: 20.125, max: 28)\n",
      "\n",
      "Run: 9, exploration: 0.838544138970058, score: 15\n",
      "Scores: (min: 14, avg: 19.555555555555557, max: 28)\n",
      "\n",
      "Run: 10, exploration: 0.8268805241487632, score: 14\n",
      "Scores: (min: 14, avg: 19, max: 28)\n",
      "\n",
      "Run: 11, exploration: 0.8153791427799216, score: 14\n",
      "Scores: (min: 14, avg: 18.545454545454547, max: 28)\n",
      "\n",
      "Run: 12, exploration: 0.8088788946494789, score: 8\n",
      "Scores: (min: 8, avg: 17.666666666666668, max: 28)\n",
      "\n",
      "Run: 13, exploration: 0.7920612314455105, score: 21\n",
      "Scores: (min: 8, avg: 17.923076923076923, max: 28)\n",
      "\n",
      "Run: 14, exploration: 0.7763695993260584, score: 20\n",
      "Scores: (min: 8, avg: 18.071428571428573, max: 28)\n",
      "\n",
      "Run: 15, exploration: 0.7655707927460921, score: 14\n",
      "Scores: (min: 8, avg: 17.8, max: 28)\n",
      "\n",
      "Run: 16, exploration: 0.7556778685553796, score: 13\n",
      "Scores: (min: 8, avg: 17.5, max: 28)\n",
      "\n",
      "Run: 17, exploration: 0.7466594429963713, score: 12\n",
      "Scores: (min: 8, avg: 17.176470588235293, max: 28)\n",
      "\n",
      "Run: 18, exploration: 0.7407070321560997, score: 8\n",
      "Scores: (min: 8, avg: 16.666666666666668, max: 28)\n",
      "\n",
      "Run: 19, exploration: 0.7340672721936974, score: 9\n",
      "Scores: (min: 8, avg: 16.263157894736842, max: 28)\n",
      "\n",
      "Run: 20, exploration: 0.7260327850203407, score: 11\n",
      "Scores: (min: 8, avg: 16, max: 28)\n",
      "\n",
      "Run: 21, exploration: 0.7180862366321393, score: 11\n",
      "Scores: (min: 8, avg: 15.761904761904763, max: 28)\n",
      "\n",
      "Run: 22, exploration: 0.704564697832001, score: 19\n",
      "Scores: (min: 8, avg: 15.909090909090908, max: 28)\n",
      "\n",
      "Run: 23, exploration: 0.6681773581762521, score: 53\n",
      "Scores: (min: 8, avg: 17.52173913043478, max: 53)\n",
      "\n",
      "Run: 24, exploration: 0.6595429797320621, score: 13\n",
      "Scores: (min: 8, avg: 17.333333333333332, max: 53)\n",
      "\n",
      "Run: 25, exploration: 0.6523241732116479, score: 11\n",
      "Scores: (min: 8, avg: 17.08, max: 53)\n",
      "\n",
      "Run: 26, exploration: 0.6426075087326283, score: 15\n",
      "Scores: (min: 8, avg: 17, max: 53)\n",
      "\n",
      "Run: 27, exploration: 0.632402542800493, score: 16\n",
      "Scores: (min: 8, avg: 16.962962962962962, max: 53)\n",
      "\n",
      "Run: 28, exploration: 0.6242304567266527, score: 13\n",
      "Scores: (min: 8, avg: 16.821428571428573, max: 53)\n",
      "\n",
      "Run: 29, exploration: 0.6186348025557711, score: 9\n",
      "Scores: (min: 8, avg: 16.551724137931036, max: 53)\n",
      "\n",
      "Run: 30, exploration: 0.6088105365789144, score: 16\n",
      "Scores: (min: 8, avg: 16.533333333333335, max: 53)\n",
      "\n",
      "Run: 31, exploration: 0.602147005002946, score: 11\n",
      "Scores: (min: 8, avg: 16.35483870967742, max: 53)\n",
      "\n",
      "Run: 32, exploration: 0.5949608504704863, score: 12\n",
      "Scores: (min: 8, avg: 16.21875, max: 53)\n",
      "\n",
      "Run: 33, exploration: 0.5872725966265356, score: 13\n",
      "Scores: (min: 8, avg: 16.12121212121212, max: 53)\n",
      "\n",
      "Run: 34, exploration: 0.57968369259211, score: 13\n",
      "Scores: (min: 8, avg: 16.029411764705884, max: 53)\n",
      "\n",
      "Run: 35, exploration: 0.5744873593512512, score: 9\n",
      "Scores: (min: 8, avg: 15.82857142857143, max: 53)\n",
      "\n",
      "Run: 36, exploration: 0.5676313011014509, score: 12\n",
      "Scores: (min: 8, avg: 15.722222222222221, max: 53)\n",
      "\n",
      "Run: 37, exploration: 0.5580583821978482, score: 17\n",
      "Scores: (min: 8, avg: 15.756756756756756, max: 53)\n",
      "\n",
      "Run: 38, exploration: 0.5502961455841082, score: 14\n",
      "Scores: (min: 8, avg: 15.710526315789474, max: 53)\n",
      "\n",
      "Run: 39, exploration: 0.54427306365317, score: 11\n",
      "Scores: (min: 8, avg: 15.58974358974359, max: 53)\n",
      "\n",
      "Run: 40, exploration: 0.5372398118510032, score: 13\n",
      "Scores: (min: 8, avg: 15.525, max: 53)\n",
      "\n",
      "Run: 41, exploration: 0.531891525167934, score: 10\n",
      "Scores: (min: 8, avg: 15.390243902439025, max: 53)\n",
      "\n",
      "Run: 42, exploration: 0.522921346063882, score: 17\n",
      "Scores: (min: 8, avg: 15.428571428571429, max: 53)\n",
      "\n",
      "Run: 43, exploration: 0.5187525878460405, score: 8\n",
      "Scores: (min: 8, avg: 15.255813953488373, max: 53)\n",
      "\n",
      "Run: 44, exploration: 0.5120491189128954, score: 13\n",
      "Scores: (min: 8, avg: 15.204545454545455, max: 53)\n",
      "\n",
      "Run: 45, exploration: 0.5074590676632879, score: 9\n",
      "Scores: (min: 8, avg: 15.066666666666666, max: 53)\n",
      "\n",
      "Run: 46, exploration: 0.5019048446041944, score: 11\n",
      "Scores: (min: 8, avg: 14.978260869565217, max: 53)\n",
      "\n",
      "Run: 47, exploration: 0.49492366792863446, score: 14\n",
      "Scores: (min: 8, avg: 14.957446808510639, max: 53)\n",
      "\n",
      "Run: 48, exploration: 0.48803959497362914, score: 14\n",
      "Scores: (min: 8, avg: 14.9375, max: 53)\n",
      "\n",
      "Run: 49, exploration: 0.48125127508828036, score: 14\n",
      "Scores: (min: 8, avg: 14.918367346938776, max: 53)\n",
      "\n",
      "Run: 50, exploration: 0.47598390063456947, score: 11\n",
      "Scores: (min: 8, avg: 14.84, max: 53)\n",
      "\n",
      "Run: 51, exploration: 0.46936326777801174, score: 14\n",
      "Scores: (min: 8, avg: 14.823529411764707, max: 53)\n",
      "\n",
      "Run: 52, exploration: 0.46469070022213765, score: 10\n",
      "Scores: (min: 8, avg: 14.73076923076923, max: 53)\n",
      "\n",
      "Run: 53, exploration: 0.4600646486360102, score: 10\n",
      "Scores: (min: 8, avg: 14.641509433962264, max: 53)\n",
      "\n",
      "Run: 54, exploration: 0.4514016473420717, score: 19\n",
      "Scores: (min: 8, avg: 14.722222222222221, max: 53)\n",
      "\n",
      "Run: 55, exploration: 0.4478030481625413, score: 8\n",
      "Scores: (min: 8, avg: 14.6, max: 53)\n",
      "\n",
      "Run: 56, exploration: 0.44201640942211684, score: 13\n",
      "Scores: (min: 8, avg: 14.571428571428571, max: 53)\n",
      "\n",
      "Run: 57, exploration: 0.4367412885669117, score: 12\n",
      "Scores: (min: 8, avg: 14.526315789473685, max: 53)\n",
      "\n",
      "Run: 58, exploration: 0.43282630302490405, score: 9\n",
      "Scores: (min: 8, avg: 14.431034482758621, max: 53)\n",
      "\n",
      "Run: 59, exploration: 0.42808894786458934, score: 11\n",
      "Scores: (min: 8, avg: 14.372881355932204, max: 53)\n",
      "\n",
      "Run: 60, exploration: 0.4229800403927701, score: 12\n",
      "Scores: (min: 8, avg: 14.333333333333334, max: 53)\n",
      "\n",
      "Run: 61, exploration: 0.4179321037441544, score: 12\n",
      "Scores: (min: 8, avg: 14.295081967213115, max: 53)\n",
      "\n",
      "Run: 62, exploration: 0.41460032563814003, score: 8\n",
      "Scores: (min: 8, avg: 14.193548387096774, max: 53)\n",
      "\n",
      "Run: 63, exploration: 0.41006245680160364, score: 11\n",
      "Scores: (min: 8, avg: 14.142857142857142, max: 53)\n",
      "\n",
      "Run: 64, exploration: 0.4063866225452039, score: 9\n",
      "Scores: (min: 8, avg: 14.0625, max: 53)\n",
      "\n",
      "Run: 65, exploration: 0.4023409950366106, score: 10\n",
      "Scores: (min: 8, avg: 14, max: 53)\n",
      "\n",
      "Run: 66, exploration: 0.398335642234492, score: 10\n",
      "Scores: (min: 8, avg: 13.93939393939394, max: 53)\n",
      "\n",
      "Run: 67, exploration: 0.3935818172430853, score: 12\n",
      "Scores: (min: 8, avg: 13.91044776119403, max: 53)\n",
      "\n",
      "Run: 68, exploration: 0.38927399944206015, score: 11\n",
      "Scores: (min: 8, avg: 13.867647058823529, max: 53)\n",
      "\n",
      "Run: 69, exploration: 0.3853987301463841, score: 10\n",
      "Scores: (min: 8, avg: 13.81159420289855, max: 53)\n",
      "\n",
      "Run: 70, exploration: 0.3815620396207659, score: 10\n",
      "Scores: (min: 8, avg: 13.757142857142858, max: 53)\n",
      "\n",
      "Run: 71, exploration: 0.3777635438089284, score: 10\n",
      "Scores: (min: 8, avg: 13.704225352112676, max: 53)\n",
      "\n",
      "Run: 72, exploration: 0.37250909354954626, score: 14\n",
      "Scores: (min: 8, avg: 13.708333333333334, max: 53)\n",
      "\n",
      "Run: 73, exploration: 0.36843192017940213, score: 11\n",
      "Scores: (min: 8, avg: 13.67123287671233, max: 53)\n",
      "\n",
      "Run: 74, exploration: 0.36512926554500874, score: 9\n",
      "Scores: (min: 8, avg: 13.608108108108109, max: 53)\n",
      "\n",
      "Run: 75, exploration: 0.36077173274200636, score: 12\n",
      "Scores: (min: 8, avg: 13.586666666666666, max: 53)\n",
      "\n",
      "Run: 76, exploration: 0.35753774467029337, score: 9\n",
      "Scores: (min: 8, avg: 13.526315789473685, max: 53)\n",
      "\n",
      "Run: 77, exploration: 0.35397841359256427, score: 10\n",
      "Scores: (min: 8, avg: 13.480519480519481, max: 53)\n",
      "\n",
      "Run: 78, exploration: 0.34488910274847373, score: 26\n",
      "Scores: (min: 8, avg: 13.64102564102564, max: 53)\n",
      "\n",
      "Run: 79, exploration: 0.34111423472584396, score: 11\n",
      "Scores: (min: 8, avg: 13.60759493670886, max: 53)\n",
      "\n",
      "Run: 80, exploration: 0.33368799612046873, score: 22\n",
      "Scores: (min: 8, avg: 13.7125, max: 53)\n",
      "\n",
      "Run: 81, exploration: 0.33036609214652046, score: 10\n",
      "Scores: (min: 8, avg: 13.666666666666666, max: 53)\n",
      "\n",
      "Run: 82, exploration: 0.32707725812456445, score: 10\n",
      "Scores: (min: 8, avg: 13.621951219512194, max: 53)\n",
      "\n",
      "Run: 83, exploration: 0.3222052939901265, score: 15\n",
      "Scores: (min: 8, avg: 13.63855421686747, max: 53)\n",
      "\n",
      "Run: 84, exploration: 0.3189977016914014, score: 10\n",
      "Scores: (min: 8, avg: 13.595238095238095, max: 53)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 85, exploration: 0.31519071309757335, score: 12\n",
      "Scores: (min: 8, avg: 13.576470588235294, max: 53)\n",
      "\n",
      "Run: 86, exploration: 0.31236531710902116, score: 9\n",
      "Scores: (min: 8, avg: 13.523255813953488, max: 53)\n",
      "\n",
      "Run: 87, exploration: 0.3095652482070879, score: 9\n",
      "Scores: (min: 8, avg: 13.471264367816092, max: 53)\n",
      "\n",
      "Run: 88, exploration: 0.3058708285836198, score: 12\n",
      "Scores: (min: 8, avg: 13.454545454545455, max: 53)\n",
      "\n",
      "Run: 89, exploration: 0.3022204989748846, score: 12\n",
      "Scores: (min: 8, avg: 13.438202247191011, max: 53)\n",
      "\n",
      "Run: 90, exploration: 0.2989126458468155, score: 11\n",
      "Scores: (min: 8, avg: 13.411111111111111, max: 53)\n",
      "\n",
      "Run: 91, exploration: 0.29593693465058934, score: 10\n",
      "Scores: (min: 8, avg: 13.373626373626374, max: 53)\n",
      "\n",
      "Run: 92, exploration: 0.2929908470157829, score: 10\n",
      "Scores: (min: 8, avg: 13.33695652173913, max: 53)\n",
      "\n",
      "Run: 93, exploration: 0.2894942299343089, score: 12\n",
      "Scores: (min: 8, avg: 13.32258064516129, max: 53)\n",
      "\n",
      "Run: 94, exploration: 0.2848968999718006, score: 16\n",
      "Scores: (min: 8, avg: 13.351063829787234, max: 53)\n",
      "\n",
      "Run: 95, exploration: 0.28206071720470993, score: 10\n",
      "Scores: (min: 8, avg: 13.31578947368421, max: 53)\n",
      "\n",
      "Run: 96, exploration: 0.2792527689768127, score: 10\n",
      "Scores: (min: 8, avg: 13.28125, max: 53)\n",
      "\n",
      "Run: 97, exploration: 0.27481807913093287, score: 16\n",
      "Scores: (min: 8, avg: 13.309278350515465, max: 53)\n",
      "\n",
      "Run: 98, exploration: 0.27235458681947705, score: 9\n",
      "Scores: (min: 8, avg: 13.26530612244898, max: 53)\n",
      "\n",
      "Run: 99, exploration: 0.26910424739696437, score: 12\n",
      "Scores: (min: 8, avg: 13.252525252525253, max: 53)\n",
      "\n",
      "Run: 100, exploration: 0.26509581755607453, score: 15\n",
      "Scores: (min: 8, avg: 13.27, max: 53)\n",
      "\n",
      "Run: 101, exploration: 0.2544414683340336, score: 41\n",
      "Scores: (min: 8, avg: 13.54, max: 53)\n",
      "\n",
      "Run: 102, exploration: 0.24593168922618383, score: 34\n",
      "Scores: (min: 8, avg: 13.74, max: 53)\n",
      "\n",
      "Run: 103, exploration: 0.23770651915214663, score: 34\n",
      "Scores: (min: 8, avg: 13.88, max: 53)\n",
      "\n",
      "Run: 104, exploration: 0.22860995245153565, score: 39\n",
      "Scores: (min: 8, avg: 13.99, max: 53)\n",
      "\n",
      "Run: 105, exploration: 0.22074314268147424, score: 35\n",
      "Scores: (min: 8, avg: 14.12, max: 53)\n",
      "\n",
      "Run: 106, exploration: 0.20622457658762192, score: 68\n",
      "Scores: (min: 8, avg: 14.52, max: 68)\n",
      "\n",
      "Run: 107, exploration: 0.19596581594477305, score: 51\n",
      "Scores: (min: 8, avg: 14.88, max: 68)\n",
      "\n",
      "Run: 108, exploration: 0.1877138485735047, score: 43\n",
      "Scores: (min: 8, avg: 15.11, max: 68)\n",
      "\n",
      "Run: 109, exploration: 0.18198116830822272, score: 31\n",
      "Scores: (min: 8, avg: 15.27, max: 68)\n",
      "\n",
      "Run: 110, exploration: 0.17103568008538103, score: 62\n",
      "Scores: (min: 8, avg: 15.75, max: 68)\n",
      "\n",
      "Run: 111, exploration: 0.16139312693113966, score: 58\n",
      "Scores: (min: 8, avg: 16.19, max: 68)\n",
      "\n",
      "Run: 112, exploration: 0.15092901825454533, score: 67\n",
      "Scores: (min: 8, avg: 16.78, max: 68)\n",
      "\n",
      "Run: 113, exploration: 0.13238893993901227, score: 131\n",
      "Scores: (min: 8, avg: 17.88, max: 131)\n",
      "\n",
      "Run: 114, exploration: 0.11966468874830531, score: 101\n",
      "Scores: (min: 8, avg: 18.69, max: 131)\n",
      "\n",
      "Run: 115, exploration: 0.11112509474506281, score: 74\n",
      "Scores: (min: 8, avg: 19.29, max: 131)\n",
      "\n",
      "Run: 116, exploration: 0.0792404922286108, score: 338\n",
      "Scores: (min: 8, avg: 22.54, max: 338)\n",
      "\n",
      "Run: 117, exploration: 0.059641124167769746, score: 284\n",
      "Scores: (min: 8, avg: 25.26, max: 338)\n",
      "\n",
      "Run: 118, exploration: 0.04565950270938761, score: 267\n",
      "Scores: (min: 8, avg: 27.85, max: 338)\n",
      "\n",
      "Run: 119, exploration: 0.037944295948747826, score: 185\n",
      "Scores: (min: 8, avg: 29.61, max: 338)\n",
      "\n",
      "Run: 120, exploration: 0.03278746046707911, score: 146\n",
      "Scores: (min: 8, avg: 30.96, max: 338)\n",
      "\n",
      "Run: 121, exploration: 0.028702367053437684, score: 133\n",
      "Scores: (min: 8, avg: 32.18, max: 338)\n",
      "\n",
      "Run: 122, exploration: 0.02312401118785413, score: 216\n",
      "Scores: (min: 8, avg: 34.15, max: 338)\n",
      "\n",
      "Run: 123, exploration: 0.01838907883180364, score: 229\n",
      "Scores: (min: 8, avg: 35.91, max: 338)\n",
      "\n",
      "Run: 124, exploration: 0.014652974771514253, score: 227\n",
      "Scores: (min: 8, avg: 38.05, max: 338)\n",
      "\n",
      "Run: 125, exploration: 0.012177022925515993, score: 185\n",
      "Scores: (min: 8, avg: 39.79, max: 338)\n",
      "\n",
      "Run: 126, exploration: 0.010038766466684862, score: 193\n",
      "Scores: (min: 8, avg: 41.57, max: 338)\n",
      "\n",
      "Run: 127, exploration: 0.009998671593271896, score: 18\n",
      "Scores: (min: 8, avg: 41.59, max: 338)\n",
      "\n",
      "Run: 128, exploration: 0.009998671593271896, score: 210\n",
      "Scores: (min: 8, avg: 43.56, max: 338)\n",
      "\n",
      "Run: 129, exploration: 0.009998671593271896, score: 365\n",
      "Scores: (min: 8, avg: 47.12, max: 365)\n",
      "\n",
      "Run: 130, exploration: 0.009998671593271896, score: 140\n",
      "Scores: (min: 8, avg: 48.36, max: 365)\n",
      "\n",
      "Run: 131, exploration: 0.009998671593271896, score: 218\n",
      "Scores: (min: 8, avg: 50.43, max: 365)\n",
      "\n",
      "Run: 132, exploration: 0.009998671593271896, score: 102\n",
      "Scores: (min: 8, avg: 51.33, max: 365)\n",
      "\n",
      "Run: 133, exploration: 0.009998671593271896, score: 116\n",
      "Scores: (min: 8, avg: 52.36, max: 365)\n",
      "\n",
      "Run: 134, exploration: 0.009998671593271896, score: 137\n",
      "Scores: (min: 8, avg: 53.6, max: 365)\n",
      "\n",
      "Run: 135, exploration: 0.009998671593271896, score: 133\n",
      "Scores: (min: 8, avg: 54.84, max: 365)\n",
      "\n",
      "Run: 136, exploration: 0.009998671593271896, score: 123\n",
      "Scores: (min: 8, avg: 55.95, max: 365)\n",
      "\n",
      "Run: 137, exploration: 0.009998671593271896, score: 126\n",
      "Scores: (min: 8, avg: 57.04, max: 365)\n",
      "\n",
      "Run: 138, exploration: 0.009998671593271896, score: 392\n",
      "Scores: (min: 8, avg: 60.82, max: 392)\n",
      "\n",
      "Run: 139, exploration: 0.009998671593271896, score: 135\n",
      "Scores: (min: 8, avg: 62.06, max: 392)\n",
      "\n",
      "Run: 140, exploration: 0.009998671593271896, score: 201\n",
      "Scores: (min: 8, avg: 63.94, max: 392)\n",
      "\n",
      "Run: 141, exploration: 0.009998671593271896, score: 190\n",
      "Scores: (min: 8, avg: 65.74, max: 392)\n",
      "\n",
      "Run: 142, exploration: 0.009998671593271896, score: 130\n",
      "Scores: (min: 8, avg: 66.87, max: 392)\n",
      "\n",
      "Run: 143, exploration: 0.009998671593271896, score: 172\n",
      "Scores: (min: 8, avg: 68.51, max: 392)\n",
      "\n",
      "Run: 144, exploration: 0.009998671593271896, score: 235\n",
      "Scores: (min: 8, avg: 70.73, max: 392)\n",
      "\n",
      "Run: 145, exploration: 0.009998671593271896, score: 397\n",
      "Scores: (min: 8, avg: 74.61, max: 397)\n",
      "\n",
      "Run: 146, exploration: 0.009998671593271896, score: 191\n",
      "Scores: (min: 8, avg: 76.41, max: 397)\n",
      "\n",
      "Run: 147, exploration: 0.009998671593271896, score: 204\n",
      "Scores: (min: 8, avg: 78.31, max: 397)\n",
      "\n",
      "Run: 148, exploration: 0.009998671593271896, score: 500\n",
      "Scores: (min: 8, avg: 83.17, max: 500)\n",
      "\n",
      "Run: 149, exploration: 0.009998671593271896, score: 238\n",
      "Scores: (min: 8, avg: 85.41, max: 500)\n",
      "\n",
      "Run: 150, exploration: 0.009998671593271896, score: 215\n",
      "Scores: (min: 8, avg: 87.45, max: 500)\n",
      "\n",
      "Run: 151, exploration: 0.009998671593271896, score: 193\n",
      "Scores: (min: 8, avg: 89.24, max: 500)\n",
      "\n",
      "Run: 152, exploration: 0.009998671593271896, score: 230\n",
      "Scores: (min: 8, avg: 91.44, max: 500)\n",
      "\n",
      "Run: 153, exploration: 0.009998671593271896, score: 288\n",
      "Scores: (min: 8, avg: 94.22, max: 500)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a175d8d8604c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# every time step do the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-d1210e464604>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mupdate_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)\n",
    "# get size of state and action from environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "score_logger = ScoreLogger(ENV_NAME)\n",
    "\n",
    "agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "scores, episodes = [], []\n",
    "run = 0\n",
    "\n",
    "\"\"\"\n",
    "Implement the noisy reward\n",
    "set the noise power noise_power\n",
    "\"\"\"\n",
    "noise_power = 0.1\n",
    "reward_space = [-1,1]\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    step = 0\n",
    "    run += 1\n",
    "    while not done:\n",
    "        step += 1\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # get action for the current state and go one step in environment\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "        #reward = reward if not done or score == 499 else -100\n",
    "        reward = reward if not done else -reward\n",
    "        \n",
    "        random_number = np.random.uniform(0,1,1)\n",
    "        if random_number < noise_power:\n",
    "            reward = -reward\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            #score = score if score == 500 else score + 100\n",
    "            #scores.append(score)\n",
    "            \n",
    "            print(\"Run: \" + str(run) + \", exploration: \" + str(agent.epsilon) + \", score: \" + str(step))\n",
    "            score_logger.add_score(step, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save(\"my_model_noisy_reward.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
