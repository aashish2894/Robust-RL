{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "#import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "from scores.score_logger import ScoreLogger\n",
    "\n",
    "EPISODES = 300\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_ddqn.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_next = self.model.predict(update_target)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_next[i])\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardPredictor:\n",
    "    \n",
    "    def __init__(self,input_shape,output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(self.input_shape,), activation=\"relu\"))\n",
    "        self.model.add(Dense(10, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.output_shape, activation=\"sigmoid\"))\n",
    "        self.model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "    \n",
    "    def fit(self,state,reward):\n",
    "        self.model.fit(state, reward, verbose=0)\n",
    "    \n",
    "    def predict(self,state):\n",
    "        reward = self.model.predict(state)\n",
    "        if reward > 0.5:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        return reward\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model.save('my_model_reward_predictor.h5')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Run: 1, exploration: 0.9851045463620021, score: 15\n",
      "Scores: (min: 15, avg: 15, max: 15)\n",
      "\n",
      "Run: 2, exploration: 0.9607702107358118, score: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aashishkumar/Documents/Projects/Robust-RL/scores/score_logger.py:77: RankWarning: Polyfit may be poorly conditioned\n",
      "  z = np.polyfit(np.array(trend_x), np.array(y[1:]), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: (min: 15, avg: 20, max: 25)\n",
      "\n",
      "Run: 3, exploration: 0.925854183751895, score: 37\n",
      "Scores: (min: 15, avg: 25.666666666666668, max: 37)\n",
      "\n",
      "Run: 4, exploration: 0.9111511025165902, score: 16\n",
      "Scores: (min: 15, avg: 23.25, max: 37)\n",
      "\n",
      "Run: 5, exploration: 0.9002772252562138, score: 12\n",
      "Scores: (min: 12, avg: 21, max: 37)\n",
      "\n",
      "Run: 6, exploration: 0.8789172357313328, score: 24\n",
      "Scores: (min: 12, avg: 21.5, max: 37)\n",
      "\n",
      "Run: 7, exploration: 0.8701675093639105, score: 10\n",
      "Scores: (min: 10, avg: 19.857142857142858, max: 37)\n",
      "\n",
      "Run: 8, exploration: 0.8606433826830369, score: 11\n",
      "Scores: (min: 10, avg: 18.75, max: 37)\n",
      "\n",
      "Run: 9, exploration: 0.8520755747117399, score: 10\n",
      "Scores: (min: 10, avg: 17.77777777777778, max: 37)\n",
      "\n",
      "Run: 10, exploration: 0.8419067177676068, score: 12\n",
      "Scores: (min: 10, avg: 17.2, max: 37)\n",
      "\n",
      "Run: 11, exploration: 0.831859218194368, score: 12\n",
      "Scores: (min: 10, avg: 16.727272727272727, max: 37)\n",
      "\n",
      "Run: 12, exploration: 0.8202885863627752, score: 14\n",
      "Scores: (min: 10, avg: 16.5, max: 37)\n",
      "\n",
      "Run: 13, exploration: 0.8048425808804326, score: 19\n",
      "Scores: (min: 10, avg: 16.692307692307693, max: 37)\n",
      "\n",
      "Run: 14, exploration: 0.7865334084168147, score: 23\n",
      "Scores: (min: 10, avg: 17.142857142857142, max: 37)\n",
      "\n",
      "Run: 15, exploration: 0.7632763763146613, score: 30\n",
      "Scores: (min: 10, avg: 18, max: 37)\n",
      "\n",
      "Run: 16, exploration: 0.7556778685553796, score: 10\n",
      "Scores: (min: 10, avg: 17.5, max: 37)\n",
      "\n",
      "Run: 17, exploration: 0.7355376118798452, score: 27\n",
      "Scores: (min: 10, avg: 18.058823529411764, max: 37)\n",
      "\n",
      "Run: 18, exploration: 0.7260327850203407, score: 13\n",
      "Scores: (min: 10, avg: 17.77777777777778, max: 37)\n",
      "\n",
      "Run: 19, exploration: 0.7130746876787832, score: 18\n",
      "Scores: (min: 10, avg: 17.789473684210527, max: 37)\n",
      "\n",
      "Run: 20, exploration: 0.704564697832001, score: 12\n",
      "Scores: (min: 10, avg: 17.5, max: 37)\n",
      "\n",
      "Run: 21, exploration: 0.6899158648955466, score: 21\n",
      "Scores: (min: 10, avg: 17.666666666666668, max: 37)\n",
      "\n",
      "Run: 22, exploration: 0.6810005752658319, score: 13\n",
      "Scores: (min: 10, avg: 17.454545454545453, max: 37)\n",
      "\n",
      "Run: 23, exploration: 0.6708567627695098, score: 15\n",
      "Scores: (min: 10, avg: 17.347826086956523, max: 37)\n",
      "\n",
      "Run: 24, exploration: 0.6621877602947683, score: 13\n",
      "Scores: (min: 10, avg: 17.166666666666668, max: 37)\n",
      "\n",
      "Run: 25, exploration: 0.6503691570122084, score: 18\n",
      "Scores: (min: 10, avg: 17.2, max: 37)\n",
      "\n",
      "Run: 26, exploration: 0.6374846057319378, score: 20\n",
      "Scores: (min: 10, avg: 17.307692307692307, max: 37)\n",
      "\n",
      "Run: 27, exploration: 0.6273609943589779, score: 16\n",
      "Scores: (min: 10, avg: 17.25925925925926, max: 37)\n",
      "\n",
      "Run: 28, exploration: 0.6112518790281489, score: 26\n",
      "Scores: (min: 10, avg: 17.571428571428573, max: 37)\n",
      "\n",
      "Run: 29, exploration: 0.6051667933504074, score: 10\n",
      "Scores: (min: 10, avg: 17.310344827586206, max: 37)\n",
      "\n",
      "Run: 30, exploration: 0.5949608504704863, score: 17\n",
      "Scores: (min: 10, avg: 17.3, max: 37)\n",
      "\n",
      "Run: 31, exploration: 0.584342100499579, score: 18\n",
      "Scores: (min: 10, avg: 17.322580645161292, max: 37)\n",
      "\n",
      "Run: 32, exploration: 0.5791040088995179, score: 9\n",
      "Scores: (min: 9, avg: 17.0625, max: 37)\n",
      "\n",
      "Run: 33, exploration: 0.5704779919833761, score: 15\n",
      "Scores: (min: 9, avg: 17, max: 37)\n",
      "\n",
      "Run: 34, exploration: 0.5530559000531677, score: 31\n",
      "Scores: (min: 9, avg: 17.41176470588235, max: 37)\n",
      "\n",
      "Run: 35, exploration: 0.547550162317433, score: 10\n",
      "Scores: (min: 9, avg: 17.2, max: 37)\n",
      "\n",
      "Run: 36, exploration: 0.5393941542601555, score: 15\n",
      "Scores: (min: 9, avg: 17.13888888888889, max: 37)\n",
      "\n",
      "Run: 37, exploration: 0.534024420840334, score: 10\n",
      "Scores: (min: 9, avg: 16.945945945945947, max: 37)\n",
      "\n",
      "Run: 38, exploration: 0.5292373811410898, score: 9\n",
      "Scores: (min: 9, avg: 16.736842105263158, max: 37)\n",
      "\n",
      "Run: 39, exploration: 0.5197916513571034, score: 18\n",
      "Scores: (min: 9, avg: 16.76923076923077, max: 37)\n",
      "\n",
      "Run: 40, exploration: 0.515132195397268, score: 9\n",
      "Scores: (min: 9, avg: 16.575, max: 37)\n",
      "\n",
      "Run: 41, exploration: 0.5079670346979859, score: 14\n",
      "Scores: (min: 9, avg: 16.51219512195122, max: 37)\n",
      "\n",
      "Run: 42, exploration: 0.5034135755936622, score: 9\n",
      "Scores: (min: 9, avg: 16.333333333333332, max: 37)\n",
      "\n",
      "Run: 43, exploration: 0.49541908701565013, score: 16\n",
      "Scores: (min: 9, avg: 16.325581395348838, max: 37)\n",
      "\n",
      "Run: 44, exploration: 0.4904871306580321, score: 10\n",
      "Scores: (min: 9, avg: 16.181818181818183, max: 37)\n",
      "\n",
      "Run: 45, exploration: 0.4836647671103911, score: 14\n",
      "Scores: (min: 9, avg: 16.133333333333333, max: 37)\n",
      "\n",
      "Run: 46, exploration: 0.47646036099556505, score: 15\n",
      "Scores: (min: 9, avg: 16.108695652173914, max: 37)\n",
      "\n",
      "Run: 47, exploration: 0.4619095170944617, score: 31\n",
      "Scores: (min: 9, avg: 16.425531914893618, max: 37)\n",
      "\n",
      "Run: 48, exploration: 0.452758565214779, score: 20\n",
      "Scores: (min: 9, avg: 16.5, max: 37)\n",
      "\n",
      "Run: 49, exploration: 0.4478030481625413, score: 11\n",
      "Scores: (min: 9, avg: 16.387755102040817, max: 37)\n",
      "\n",
      "Run: 50, exploration: 0.44334511517564335, score: 10\n",
      "Scores: (min: 9, avg: 16.26, max: 37)\n",
      "\n",
      "Run: 51, exploration: 0.4389315614456469, score: 10\n",
      "Scores: (min: 9, avg: 16.137254901960784, max: 37)\n",
      "\n",
      "Run: 52, exploration: 0.43325956258749154, score: 13\n",
      "Scores: (min: 9, avg: 16.076923076923077, max: 37)\n",
      "\n",
      "Run: 53, exploration: 0.4263791588948904, score: 16\n",
      "Scores: (min: 9, avg: 16.07547169811321, max: 37)\n",
      "\n",
      "Run: 54, exploration: 0.41501534097911913, score: 27\n",
      "Scores: (min: 9, avg: 16.27777777777778, max: 37)\n",
      "\n",
      "Run: 55, exploration: 0.41088381354487985, score: 10\n",
      "Scores: (min: 9, avg: 16.163636363636364, max: 37)\n",
      "\n",
      "Run: 56, exploration: 0.4072006165777428, score: 9\n",
      "Scores: (min: 9, avg: 16.035714285714285, max: 37)\n",
      "\n",
      "Run: 57, exploration: 0.4035504360971442, score: 9\n",
      "Scores: (min: 9, avg: 15.912280701754385, max: 37)\n",
      "\n",
      "Run: 58, exploration: 0.3987343766111031, score: 12\n",
      "Scores: (min: 9, avg: 15.844827586206897, max: 37)\n",
      "\n",
      "Run: 59, exploration: 0.3939757930361214, score: 12\n",
      "Scores: (min: 9, avg: 15.779661016949152, max: 37)\n",
      "\n",
      "Run: 60, exploration: 0.3873315182940503, score: 17\n",
      "Scores: (min: 9, avg: 15.8, max: 37)\n",
      "\n",
      "Run: 61, exploration: 0.3827090189332178, score: 12\n",
      "Scores: (min: 9, avg: 15.737704918032787, max: 37)\n",
      "\n",
      "Run: 62, exploration: 0.379278383188116, score: 9\n",
      "Scores: (min: 9, avg: 15.629032258064516, max: 37)\n",
      "\n",
      "Run: 63, exploration: 0.3747519917093475, score: 12\n",
      "Scores: (min: 9, avg: 15.571428571428571, max: 37)\n",
      "\n",
      "Run: 64, exploration: 0.37102129074024554, score: 10\n",
      "Scores: (min: 9, avg: 15.484375, max: 37)\n",
      "\n",
      "noise remover activated\n",
      "Run: 65, exploration: 0.36732772934619257, score: 10\n",
      "Scores: (min: 9, avg: 15.4, max: 37)\n",
      "\n",
      "Run: 66, exploration: 0.36185621618376534, score: 15\n",
      "Scores: (min: 9, avg: 15.393939393939394, max: 37)\n",
      "\n",
      "Run: 67, exploration: 0.3586125067115203, score: 9\n",
      "Scores: (min: 9, avg: 15.298507462686567, max: 37)\n",
      "\n",
      "Run: 68, exploration: 0.35539787412304774, score: 9\n",
      "Scores: (min: 9, avg: 15.205882352941176, max: 37)\n",
      "\n",
      "Run: 69, exploration: 0.3494042035469346, score: 17\n",
      "Scores: (min: 9, avg: 15.231884057971014, max: 37)\n",
      "\n",
      "Run: 70, exploration: 0.34557991700256185, score: 11\n",
      "Scores: (min: 9, avg: 15.17142857142857, max: 37)\n",
      "\n",
      "Run: 71, exploration: 0.340432347370627, score: 15\n",
      "Scores: (min: 9, avg: 15.169014084507042, max: 37)\n",
      "\n",
      "Run: 72, exploration: 0.3370433025720758, score: 10\n",
      "Scores: (min: 9, avg: 15.097222222222221, max: 37)\n",
      "\n",
      "Run: 73, exploration: 0.33335430812434824, score: 11\n",
      "Scores: (min: 9, avg: 15.04109589041096, max: 37)\n",
      "\n",
      "Run: 74, exploration: 0.3300357260543739, score: 10\n",
      "Scores: (min: 9, avg: 14.972972972972974, max: 37)\n",
      "\n",
      "Run: 75, exploration: 0.32707725812456445, score: 9\n",
      "Scores: (min: 9, avg: 14.893333333333333, max: 37)\n",
      "\n",
      "Run: 76, exploration: 0.324469779929346, score: 8\n",
      "Scores: (min: 8, avg: 14.802631578947368, max: 37)\n",
      "\n",
      "Run: 77, exploration: 0.3212396444018328, score: 10\n",
      "Scores: (min: 8, avg: 14.74025974025974, max: 37)\n",
      "\n",
      "Run: 78, exploration: 0.31645463417195824, score: 15\n",
      "Scores: (min: 8, avg: 14.743589743589743, max: 37)\n",
      "\n",
      "Run: 79, exploration: 0.31330429038059615, score: 10\n",
      "Scores: (min: 8, avg: 14.683544303797468, max: 37)\n",
      "\n",
      "Run: 80, exploration: 0.30987512333041833, score: 11\n",
      "Scores: (min: 8, avg: 14.6375, max: 37)\n",
      "\n",
      "Run: 81, exploration: 0.3064834890782873, score: 11\n",
      "Scores: (min: 8, avg: 14.592592592592593, max: 37)\n",
      "\n",
      "Run: 82, exploration: 0.3034324092307786, score: 10\n",
      "Scores: (min: 8, avg: 14.536585365853659, max: 37)\n",
      "\n",
      "Run: 83, exploration: 0.29981118025384745, score: 12\n",
      "Scores: (min: 8, avg: 14.506024096385541, max: 37)\n",
      "\n",
      "Run: 84, exploration: 0.29742106875640767, score: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: (min: 8, avg: 14.428571428571429, max: 37)\n",
      "\n",
      "Run: 85, exploration: 0.2935777088557856, score: 13\n",
      "Scores: (min: 8, avg: 14.411764705882353, max: 37)\n",
      "\n",
      "Run: 86, exploration: 0.2912372909409696, score: 8\n",
      "Scores: (min: 8, avg: 14.337209302325581, max: 37)\n",
      "\n",
      "Run: 87, exploration: 0.28776160118260813, score: 12\n",
      "Scores: (min: 8, avg: 14.310344827586206, max: 37)\n",
      "\n",
      "Run: 88, exploration: 0.284327391068757, score: 12\n",
      "Scores: (min: 8, avg: 14.284090909090908, max: 37)\n",
      "\n",
      "Run: 89, exploration: 0.28093416557223355, score: 12\n",
      "Scores: (min: 8, avg: 14.258426966292134, max: 37)\n",
      "\n",
      "Run: 90, exploration: 0.2781374323007875, score: 10\n",
      "Scores: (min: 8, avg: 14.21111111111111, max: 37)\n",
      "\n",
      "Run: 91, exploration: 0.2734467341692626, score: 17\n",
      "Scores: (min: 8, avg: 14.241758241758241, max: 37)\n",
      "\n",
      "Run: 92, exploration: 0.26910424739696437, score: 16\n",
      "Scores: (min: 8, avg: 14.26086956521739, max: 37)\n",
      "\n",
      "Run: 93, exploration: 0.26536117873480936, score: 14\n",
      "Scores: (min: 8, avg: 14.258064516129032, max: 37)\n",
      "\n",
      "Run: 94, exploration: 0.2629824588716937, score: 9\n",
      "Scores: (min: 8, avg: 14.202127659574469, max: 37)\n",
      "\n",
      "Run: 95, exploration: 0.253425228091996, score: 37\n",
      "Scores: (min: 8, avg: 14.442105263157895, max: 37)\n",
      "\n",
      "Run: 96, exploration: 0.25065144723887983, score: 11\n",
      "Scores: (min: 8, avg: 14.40625, max: 37)\n",
      "\n",
      "Run: 97, exploration: 0.24840458664265946, score: 9\n",
      "Scores: (min: 8, avg: 14.350515463917526, max: 37)\n",
      "\n",
      "Run: 98, exploration: 0.24593168922618383, score: 10\n",
      "Scores: (min: 8, avg: 14.306122448979592, max: 37)\n",
      "\n",
      "Run: 99, exploration: 0.2432399263899185, score: 11\n",
      "Scores: (min: 8, avg: 14.272727272727273, max: 37)\n",
      "\n",
      "Run: 100, exploration: 0.24009671066809293, score: 13\n",
      "Scores: (min: 8, avg: 14.26, max: 37)\n",
      "\n",
      "Run: 101, exploration: 0.23770651915214663, score: 10\n",
      "Scores: (min: 8, avg: 14.21, max: 37)\n",
      "\n",
      "Run: 102, exploration: 0.2346348076972527, score: 13\n",
      "Scores: (min: 8, avg: 14.09, max: 37)\n",
      "\n",
      "Run: 103, exploration: 0.23206669108958414, score: 11\n",
      "Scores: (min: 8, avg: 13.83, max: 37)\n",
      "\n",
      "Run: 104, exploration: 0.22975643938045995, score: 10\n",
      "Scores: (min: 8, avg: 13.77, max: 37)\n",
      "\n",
      "Run: 105, exploration: 0.22769688338723304, score: 9\n",
      "Scores: (min: 8, avg: 13.74, max: 37)\n",
      "\n",
      "Run: 106, exploration: 0.2247545193013052, score: 13\n",
      "Scores: (min: 8, avg: 13.63, max: 37)\n",
      "\n",
      "Run: 107, exploration: 0.22140669877912197, score: 15\n",
      "Scores: (min: 8, avg: 13.68, max: 37)\n",
      "\n",
      "Run: 108, exploration: 0.20437596218618437, score: 80\n",
      "Scores: (min: 8, avg: 14.37, max: 80)\n",
      "\n",
      "Run: 109, exploration: 0.197343080201605, score: 35\n",
      "Scores: (min: 8, avg: 14.62, max: 80)\n",
      "\n",
      "Run: 110, exploration: 0.1862173832738307, score: 58\n",
      "Scores: (min: 8, avg: 15.08, max: 80)\n",
      "\n",
      "Run: 111, exploration: 0.18089200737900324, score: 29\n",
      "Scores: (min: 8, avg: 15.25, max: 80)\n",
      "\n",
      "Run: 112, exploration: 0.16916366640232217, score: 67\n",
      "Scores: (min: 8, avg: 15.78, max: 80)\n",
      "\n",
      "Run: 113, exploration: 0.1607485221367656, score: 51\n",
      "Scores: (min: 8, avg: 16.1, max: 80)\n",
      "\n",
      "Run: 114, exploration: 0.1533645326385287, score: 47\n",
      "Scores: (min: 8, avg: 16.34, max: 80)\n",
      "\n",
      "Run: 115, exploration: 0.1486808620605061, score: 31\n",
      "Scores: (min: 8, avg: 16.35, max: 80)\n",
      "\n",
      "Run: 116, exploration: 0.14185120098835316, score: 47\n",
      "Scores: (min: 8, avg: 16.72, max: 80)\n",
      "\n",
      "Run: 117, exploration: 0.12196121415881125, score: 151\n",
      "Scores: (min: 8, avg: 17.96, max: 151)\n",
      "\n",
      "Run: 118, exploration: 0.1116823909923804, score: 88\n",
      "Scores: (min: 8, avg: 18.71, max: 151)\n",
      "\n",
      "Run: 119, exploration: 0.09234799400196655, score: 190\n",
      "Scores: (min: 8, avg: 20.43, max: 190)\n",
      "\n",
      "Run: 120, exploration: 0.08592956600102684, score: 72\n",
      "Scores: (min: 8, avg: 21.03, max: 190)\n",
      "\n",
      "Run: 121, exploration: 0.07806016915036741, score: 96\n",
      "Scores: (min: 8, avg: 21.78, max: 190)\n",
      "\n",
      "Run: 122, exploration: 0.0629519890290139, score: 215\n",
      "Scores: (min: 8, avg: 23.8, max: 215)\n",
      "\n",
      "Run: 123, exploration: 0.05747366406386194, score: 91\n",
      "Scores: (min: 8, avg: 24.56, max: 215)\n",
      "\n",
      "Run: 124, exploration: 0.04843589976985391, score: 171\n",
      "Scores: (min: 8, avg: 26.14, max: 215)\n",
      "\n",
      "Run: 125, exploration: 0.04135371320577681, score: 158\n",
      "Scores: (min: 8, avg: 27.54, max: 215)\n",
      "\n",
      "Run: 126, exploration: 0.03426307193188217, score: 188\n",
      "Scores: (min: 8, avg: 29.22, max: 215)\n",
      "\n",
      "Run: 127, exploration: 0.025736881956867325, score: 286\n",
      "Scores: (min: 8, avg: 31.92, max: 286)\n",
      "\n",
      "Run: 128, exploration: 0.0209853167478185, score: 204\n",
      "Scores: (min: 8, avg: 33.7, max: 286)\n",
      "\n",
      "Run: 129, exploration: 0.01601759652309025, score: 270\n",
      "Scores: (min: 8, avg: 36.3, max: 286)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3bc30d6702f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mreward_noise_remover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_noise_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5210aab9531b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, state, reward)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2658\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2659\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "env = gym.make(ENV_NAME)\n",
    "# get size of state and action from environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "score_logger = ScoreLogger(ENV_NAME)\n",
    "\n",
    "agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "reward_noise_remover = RewardPredictor(state_size + 1,1)\n",
    "\n",
    "scores, episodes = [], []\n",
    "run = 0\n",
    "\n",
    "\"\"\"\n",
    "Implement the noisy reward\n",
    "set the noise power noise_power\n",
    "\"\"\"\n",
    "noise_power = 0.1\n",
    "reward_space = [-1,1]\n",
    "\n",
    "counter_to_use_reward = 0\n",
    "dont_count = False\n",
    "print_flag = True\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    step = 0\n",
    "    run += 1\n",
    "    while not done:\n",
    "        step += 1\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # get action for the current state and go one step in environment\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # if an action make the episode end, then gives penalty of -100\n",
    "        #reward = reward if not done or score == 499 else -100\n",
    "        reward = reward if not done else -reward\n",
    "        \n",
    "        random_number = np.random.uniform(0,1,1)\n",
    "        if random_number < noise_power:\n",
    "            reward = -reward\n",
    "        \n",
    "        ## Noise Filtering Process\n",
    "        \n",
    "        reward_noise_state = state\n",
    "        reward_noise_state = np.concatenate((reward_noise_state,action),axis=None)\n",
    "        reward_noise_state = np.reshape(reward_noise_state, [1, state_size+1])\n",
    "        #print(reward_noise_state.shape)\n",
    "        ## Filter noise\n",
    "        old_reward = 1 if reward > 0 else 0\n",
    "        old_reward = np.reshape(old_reward, [1, 1])\n",
    "        if not dont_count :\n",
    "            counter_to_use_reward += 1\n",
    "        if counter_to_use_reward > 1000:\n",
    "            if print_flag:\n",
    "                print('noise remover activated')\n",
    "                print_flag = False\n",
    "            dont_count = True\n",
    "            reward = reward_noise_remover.predict(reward_noise_state)\n",
    "        \n",
    "        ##\n",
    "        reward_noise_remover.fit(reward_noise_state, old_reward)\n",
    "        \n",
    "        ########\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        # every time step do the training\n",
    "        agent.train_model()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # every episode, plot the play time\n",
    "            #score = score if score == 500 else score + 100\n",
    "            #scores.append(score)\n",
    "            \n",
    "            print(\"Run: \" + str(run) + \", exploration: \" + str(agent.epsilon) + \", score: \" + str(step))\n",
    "            score_logger.add_score(step, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 25 noise remover activated\n",
    "# run 67\n",
    "# run 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save(\"my_model_filtered_noisy_reward.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_noise_remover.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
